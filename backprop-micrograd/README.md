This project is a follow-along implementation of [Andrej Karpathy's micrograd](https://github.com/karpathy/micrograd) â€” a tiny autograd engine (with a scalar-valued reverse-mode autodiff) and a toy neural net library, all from scratch in Python.

## ðŸš€ About

The goal of this repo is to:
- Learn and understand how backprop, reverse-mode autodiff works.
- Build intuition around neural network training fundamentals without relying on libraries.

## ðŸ“š Original Reference

This implementation is inspired by and closely follows:  
ðŸ‘‰ [karpathy/micrograd](https://github.com/karpathy/micrograd)

> If you're looking for the original, go support Karpathyâ€™s work and check out the beautifully written [original repo](https://github.com/karpathy/micrograd).